{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ad33c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "MODEL_PATH = \"../models/Phi-3-mini-4k-instruct-Q4_K_M/Phi-3-mini-4k-instruct-Q4_K_M.gguf\"\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=MODEL_PATH,\n",
    "    n_ctx=4096,        # full context của Phi-3 Mini\n",
    "    n_threads=12,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "630b8c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chunks = [\n",
    "    \"AI là lĩnh vực khoa học máy tính nghiên cứu cách tạo ra hệ thống thông minh có khả năng học hỏi, suy luận và tự giải quyết vấn đề tương tự như trí tuệ con người.\",\n",
    "    \"Các ứng dụng phổ biến của AI bao gồm nhận dạng hình ảnh, xử lý ngôn ngữ tự nhiên, hệ thống gợi ý cá nhân hóa và điều khiển xe tự hành trong giao thông.\",\n",
    "    \"Hạn chế của AI hiện nay gồm phụ thuộc dữ liệu lớn để huấn luyện, chi phí tính toán cao và khả năng giải thích các quyết định phức tạp còn hạn chế (vấn đề hộp đen).\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed765217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== GENERATING ANSWER =====\n",
      "\n",
      " AI là một lĩnh vực khoa học mạch lạc, thể hiện trong các hệ thống thông minh có khả năng học hỏi,"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 70\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===== GENERATING ANSWER =====\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m llm(\n\u001b[0;32m     71\u001b[0m     prompt,\n\u001b[0;32m     72\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[0;32m     73\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m,\n\u001b[0;32m     74\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[0;32m     75\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     76\u001b[0m ):\n\u001b[0;32m     77\u001b[0m     token \u001b[38;5;241m=\u001b[39m chunk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token:\n",
      "File \u001b[1;32md:\\AGU\\KhoaLuan\\E-Egent\\lab\\.venv\\lib\\site-packages\\llama_cpp\\llama.py:1322\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1320\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1321\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1322\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m   1323\u001b[0m     prompt_tokens,\n\u001b[0;32m   1324\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m   1325\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   1326\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[0;32m   1327\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[0;32m   1328\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m   1329\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[0;32m   1330\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[0;32m   1331\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[0;32m   1332\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[0;32m   1333\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[0;32m   1334\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[0;32m   1335\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[0;32m   1336\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1337\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1338\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m   1339\u001b[0m ):\n\u001b[0;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llama_cpp\u001b[38;5;241m.\u001b[39mllama_token_is_eog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mvocab, token):\n\u001b[0;32m   1341\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens, prev_tokens\u001b[38;5;241m=\u001b[39mprompt_tokens)\n",
      "File \u001b[1;32md:\\AGU\\KhoaLuan\\E-Egent\\lab\\.venv\\lib\\site-packages\\llama_cpp\\llama.py:914\u001b[0m, in \u001b[0;36mLlama.generate\u001b[1;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 914\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[0;32m    916\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[0;32m    917\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m    918\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    932\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[0;32m    933\u001b[0m         )\n",
      "File \u001b[1;32md:\\AGU\\KhoaLuan\\E-Egent\\lab\\.venv\\lib\\site-packages\\llama_cpp\\llama.py:648\u001b[0m, in \u001b[0;36mLlama.eval\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    644\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[0;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[0;32m    646\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logits_all\n\u001b[0;32m    647\u001b[0m )\n\u001b[1;32m--> 648\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[1;32md:\\AGU\\KhoaLuan\\E-Egent\\lab\\.venv\\lib\\site-packages\\llama_cpp\\_internals.py:322\u001b[0m, in \u001b[0;36mLlamaContext.decode\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: LlamaBatch):\n\u001b[1;32m--> 322\u001b[0m     return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# ==============================\n",
    "# INPUT: RAG CHUNKS\n",
    "# ==============================\n",
    "rag_chunks = [\n",
    "    \"AI là lĩnh vực khoa học máy tính nghiên cứu cách tạo ra hệ thống thông minh có khả năng học hỏi, suy luận và tự giải quyết vấn đề tương tự như trí tuệ con người.\",\n",
    "    \"Các ứng dụng phổ biến của AI bao gồm nhận dạng hình ảnh, xử lý ngôn ngữ tự nhiên, hệ thống gợi ý cá nhân hóa và điều khiển xe tự hành trong giao thông.\",\n",
    "    \"Hạn chế của AI hiện nay gồm phụ thuộc dữ liệu lớn để huấn luyện, chi phí tính toán cao và khả năng giải thích các quyết định phức tạp còn hạn chế (vấn đề hộp đen).\"\n",
    "]\n",
    "\n",
    "# ==============================\n",
    "# BUILD RAG PROMPT\n",
    "# ==============================\n",
    "prompt = f\"\"\"\n",
    "<|user|>\n",
    "Dựa trên các tài liệu tham khảo dưới đây, hãy:\n",
    "- Giải thích lại bằng tiếng Việt chuẩn, mạch lạc, học thuật\n",
    "- Có thể diễn đạt lại cho rõ nghĩa nhưng không thêm thông tin ngoài tài liệu\n",
    "\n",
    "### TÀI LIỆU THAM KHẢO\n",
    "[1] {rag_chunks[0]}\n",
    "[2] {rag_chunks[1]}\n",
    "[3] {rag_chunks[2]}\n",
    "\n",
    "### CÂU HỎI\n",
    "AI là gì? Nêu ví dụ, ứng dụng thực tế và các hạn chế.\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "# ==============================\n",
    "# HELPER FUNCTIONS\n",
    "# ==============================\n",
    "def ends_with_sentence(text):\n",
    "    return text.strip().endswith((\".\", \"!\", \"?\"))\n",
    "\n",
    "def judge_answer(llm, answer):\n",
    "    judge_prompt = f\"\"\"\n",
    "<|user|>\n",
    "Đánh giá câu trả lời sau đã ĐẦY ĐỦ hay CHƯA.\n",
    "\n",
    "Tiêu chí:\n",
    "1. Có định nghĩa AI\n",
    "2. Có ứng dụng thực tế\n",
    "3. Có hạn chế của AI\n",
    "\n",
    "Chỉ trả lời đúng một từ:\n",
    "YES hoặc NO.\n",
    "\n",
    "### CÂU TRẢ LỜI\n",
    "{answer}\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    out = llm(\n",
    "        judge_prompt,\n",
    "        max_tokens=5,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    verdict = out[\"choices\"][0][\"text\"].strip().upper()\n",
    "    print(f\"\\n[JUDGE] => {verdict}\\n\")\n",
    "    return verdict == \"YES\"\n",
    "\n",
    "# ==============================\n",
    "# STEP 1: GENERATE + STREAMING\n",
    "# ==============================\n",
    "print(\"===== GENERATING ANSWER =====\\n\")\n",
    "\n",
    "response = \"\"\n",
    "\n",
    "for chunk in llm(\n",
    "    prompt,\n",
    "    max_tokens=1024,\n",
    "    temperature=0.3,\n",
    "    top_p=0.9,\n",
    "    stream=True\n",
    "):\n",
    "    token = chunk[\"choices\"][0][\"text\"]\n",
    "    if token:\n",
    "        response += token\n",
    "        sys.stdout.write(token)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "# ==============================\n",
    "# STEP 2: AUTO-CONTINUE TO FINISH SENTENCE\n",
    "# ==============================\n",
    "if not ends_with_sentence(response):\n",
    "    continue_prompt = f\"\"\"\n",
    "<|user|>\n",
    "Hãy hoàn thành câu cuối cùng sau cho trọn vẹn, không thêm ý mới:\n",
    "\n",
    "{response}\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    for chunk in llm(\n",
    "        continue_prompt,\n",
    "        max_tokens=64,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        stream=True\n",
    "    ):\n",
    "        token = chunk[\"choices\"][0][\"text\"]\n",
    "        if token:\n",
    "            response += token\n",
    "            sys.stdout.write(token)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "# ==============================\n",
    "# STEP 3: LLM-AS-A-JUDGE\n",
    "# ==============================\n",
    "if not judge_answer(llm, response):\n",
    "    print(\"[INFO] Câu trả lời chưa đủ, đang bổ sung phần còn thiếu...\\n\")\n",
    "\n",
    "    supplement_prompt = f\"\"\"\n",
    "<|user|>\n",
    "Câu trả lời sau CHƯA đầy đủ.\n",
    "Hãy bổ sung PHẦN CÒN THIẾU để đáp ứng đủ:\n",
    "- định nghĩa\n",
    "- ứng dụng\n",
    "- hạn chế\n",
    "\n",
    "KHÔNG lặp lại nội dung đã có.\n",
    "\n",
    "### CÂU TRẢ LỜI HIỆN TẠI\n",
    "{response}\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "    for chunk in llm(\n",
    "        supplement_prompt,\n",
    "        max_tokens=256,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        stream=True\n",
    "    ):\n",
    "        token = chunk[\"choices\"][0][\"text\"]\n",
    "        if token:\n",
    "            response += token\n",
    "            sys.stdout.write(token)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "# ==============================\n",
    "# FINAL OUTPUT\n",
    "# ==============================\n",
    "print(\"\\n\\n===== FINAL ANSWER (COMPLETE) =====\\n\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc468b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ends_with_sentence(text):\n",
    "    return text.strip().endswith((\".\", \"!\", \"?\"))\n",
    "\n",
    "response = \"\"\n",
    "\n",
    "# lượt sinh chính\n",
    "for chunk in llm(prompt, max_tokens=1024, stream=True):\n",
    "    token = chunk[\"choices\"][0][\"text\"]\n",
    "    if token:\n",
    "        response += token\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "# nếu chưa tròn câu → gọi tiếp\n",
    "if not ends_with_sentence(response):\n",
    "    fix_prompt = f\"\"\"\n",
    "<|user|>\n",
    "Hãy hoàn thành câu sau cho trọn vẹn, không thêm ý mới:\n",
    "{response}\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    for chunk in llm(fix_prompt, max_tokens=64, stream=True):\n",
    "        token = chunk[\"choices\"][0][\"text\"]\n",
    "        if token:\n",
    "            print(token, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lab-venv)",
   "language": "python",
   "name": "lab-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
